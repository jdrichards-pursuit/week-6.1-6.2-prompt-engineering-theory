{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 6.1: Prompt Engineering\n",
    "\n",
    "## Retrieving Your API Key\n",
    "\n",
    "Before we begin, you will need to retrieve your API key from Gemini.\n",
    "\n",
    "Use the following set of instructions to sign up for an account and retrieve your API key.\n",
    "\n",
    "[Gemini API Key](https://github.com/jdrichards-pursuit/gemini-api-key-acquire?tab=readme-ov-file)\n",
    "\n",
    "## Setting Up Your Environment Variables\n",
    "\n",
    "Now that you have your API key, you can set up your environment variables.\n",
    "\n",
    "Create a new file called `.env` and add the following line of code:\n",
    "\n",
    "```bash\n",
    "API_KEY=<your-api-key>\n",
    "```\n",
    "\n",
    "## Installing Required Libraries\n",
    "\n",
    "Now that you have your API key, you can install the required libraries.\n",
    "\n",
    "```bash\n",
    "pip install python-dotenv\n",
    "```\n",
    "\n",
    "## Importing Required Libraries\n",
    "\n",
    "Now that you have your API key, you can import the required libraries. When you run this code, it will print your API key to the console.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "api_key = os.getenv(\"API_KEY\")\n",
    "\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After viewing your API key, you may now want to remove the print statement for security reasons.\n",
    "\n",
    "## Importing Required Libraries\n",
    "\n",
    "For this lesson you will also need to install the following libraries:\n",
    "\n",
    "```bash\n",
    "pip install google-generativeai pandas\n",
    "```\n",
    "\n",
    "## Import Packages\n",
    "\n",
    "Next, import these packages into your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up API Key\n",
    "\n",
    "You will need to set up your API key using your already created environment variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['API_KEY'] = api_key\n",
    "genai.configure(api_key=os.environ['API_KEY'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Process Under the Hood\n",
    "\n",
    "Before we begin digging into prompt engineering, let's familiarize ourselves on a high level with what happens under the hood when we send a prompt. This is definitely 'above our paygrade' and not what we will be focusing on, but it is good to be aware of. Take a look at this link which describes [The Process of A Prompt](https://github.com/jdrichards-pursuit/prompt-process-explained).\n",
    "\n",
    "## Parts of a Prompt\n",
    "\n",
    "When creating a prompt, there are three main parts to consider:\n",
    "\n",
    "1. **Model**: The model you are using to generate the response.\n",
    "2. **System Prompt**: The system prompt is the prompt that is sent to the model. It is used to guide the model's response. (Note: Gemini doesn't have an explicit system prompt, but we are faking this to emulate OpenAI's system prompt)\n",
    "3. **Input Prompt**: The input prompt is the prompt that is sent to the model. It is used to generate the response.\n",
    "\n",
    "## Model\n",
    "For this lesson, we will be using the `gemini-1.5-flash` model because it is free and '...is a fast and versatile multimodal model for scaling across diverse tasks.' to quote the [Gemini API Documentation](https://ai.google.dev/gemini-api/docs/models/gemini#gemini-1.5-flash).\n",
    "\n",
    "## System Prompt\n",
    "\n",
    "The system prompt is a prompt that is used in OpenAI's API. It is a prompt that precedes the input prompt when sent to the model.\n",
    "The purpose of the system prompt is to guide the model's response. We are faking this to emulate OpenAI's system prompt. In Gemini API, the system prompt is implicit and can be included in the input prompt. However, it should still precede the input prompt. In Claude API, this prompt is explicit using the `Human` keyword.\n",
    "\n",
    "## Input Prompt\n",
    "\n",
    "The input prompt is the prompt that is sent to the model. It is used to generate the response. This prompt comes from a user or another system or a response from the model itself.\n",
    "\n",
    "Let's take a look at an example of a prompt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are Rewind Rhonda, the enthusiastic and\n",
    "    slightly eccentric clerk at 'Blockbuster 2049', \n",
    "    a retro-futuristic video store that specializes in both \n",
    "    classic films and cutting-edge holographic experiences. \n",
    "    Your knowledge spans cinema history and you have a \n",
    "    knack for making unexpected connections between movies. \n",
    "    You're known for your witty movie puns and your ability \n",
    "    to find the perfect film for any customer's mood or occasion.\"\"\"\n",
    "\n",
    "input_prompt = \"\"\"Hey Rhonda! I'm looking for a movie night recommendation. \n",
    "    I want something with action, but also a bit of humor. \n",
    "Oh, and I love anything with time travel. \n",
    "What've you got for me?\"\"\"\n",
    "\n",
    "\n",
    "prompt = f\"{system_prompt}\\n\\n{input_prompt}\"\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet demonstrates how to construct a prompt for an AI language model, specifically for a character-based interaction. Let's break it down:\n",
    "\n",
    "1. system_prompt: This is a string that defines the character and context for the AI's responses. In this case, it's creating a persona called \"Rewind Rhonda,\" who works at a futuristic video store. This prompt gives the AI a specific role to play and background information to use in its responses.\n",
    "\n",
    "2. input_prompt: This is the actual question or request from the user. Here, someone is asking Rhonda for a movie recommendation with specific criteria (action, humor, and time travel).\n",
    "\n",
    "3. prompt: This combines the system_prompt and input_prompt into a single string. The f before the string makes it an f-string (formatted string literal), which allows us to embed expressions inside string literals using curly braces {}.\n",
    "\n",
    "4. The \\n\\n between the two prompts adds two newline characters, creating a visual separation between the system prompt and the user input.\n",
    "\n",
    "5. This structure is commonly used when working with AI language models to provide context (the system prompt) and then the specific input or question (the input prompt). It helps guide the AI to respond in a particular way or with a specific persona."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "When we send a prompt to the model, we can also send parameters to the model. These parameters are used to guide the model's response. In Gemini API, there are several parameters we can send to the model, but for now we will focus on the following:\n",
    "\n",
    "- `temperature`: This parameter controls the randomness of the model's response. A temperature of 0 is deterministic, meaning the model will always return the same response for a given prompt. A temperature between 0 and 1 will make the response more random.\n",
    "- `max_output_tokens`: This parameter controls the maximum number of tokens in the model's response.\n",
    "- `num_return_sequences`: This parameter controls the number of alternative completions to generate.\n",
    "- `min_length`: This parameter controls the minimum number of tokens in the model's response.\n",
    "- `max_length`: This parameter controls the maximum number of tokens in the model's response.\n",
    "\n",
    "\n",
    "There is also a `top_p` and `top_k` parameter, but we will not focus on those for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature\n",
    "\n",
    "- This parameter controls the randomness of the generated text.\n",
    "\n",
    "- Think of it like adjusting the \"creativity\" or \"originality\" of the generated text.\n",
    "\n",
    "- A higher temperature (e.g., 0.9) means the API will generate more unique and creative text, but it might also be less coherent or grammatically correct.\n",
    "\n",
    "- A lower temperature (e.g., 0.1) means the API will generate more predictable and coherent text, but it might also be less creative or original.\n",
    "By default, the API uses a temperature of 0.5, which is a good balance between creativity and coherence.\n",
    "\n",
    "- By default, the API uses a temperature of 0.5, which is a good balance between creativity and coherence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Output Tokens\n",
    "\n",
    "- This parameter sets the maximum number of tokens in the generated text.\n",
    "- A token is the smallest unit of text that the model can generate.\n",
    "- If you set max_output_tokens to 20, the API will generate text that's around 20 tokens long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num Return Sequences\n",
    "\n",
    "- This parameter sets the number of alternative completions to generate.\n",
    "- If you set num_return_sequences to 3, the API will generate 3 different completions for the same prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Min Length\n",
    "\n",
    "- This parameter sets the minimum number of characters (letters, spaces, punctuation) in the generated text.\n",
    "- Think of it like setting a word count for a minimum number of sentences. You might ask someone to write at least 5 sentences.\n",
    "- The API will generate text that's as close to the minimum length as possible, but it might not reach the exact limit.\n",
    "- If you set min_length to 20, the API will generate text that's around 20 characters long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Length\n",
    "\n",
    "- This parameter sets the maximum number of characters (letters, spaces, punctuation) in the generated text.\n",
    "- Think of it like setting a word count for a maximum number of sentences. You might ask someone to write at most 5 sentences.\n",
    "- The API will generate text that's as close to the maximum length as possible, but it might not reach the exact limit.\n",
    "- If you set max_length to 20, the API will generate text that's around 20 characters long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Parameters\n",
    "You do not have to set all of these parameters, but it is good to be aware of them. When they are not specified, the API will use the default values. The defaults are:\n",
    "\n",
    "- temperature: 0.7\n",
    "- max_output_tokens: 2048\n",
    "- top_p: 1\n",
    "- top_k: 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's take a look at an example of a prompt with parameters. We've already created a prompt, so we will now create a function that takes in any prompt and arguments called `kwargs`, and then we will print the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gemini-1.5-flash\", **kwargs):\n",
    "    model = genai.GenerativeModel(model)\n",
    "    \n",
    "    # Create a generation_config dictionary with default values\n",
    "    generation_config = {\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_output_tokens\": 2048,\n",
    "        \"top_p\": 1,\n",
    "        \"top_k\": 32\n",
    "    }\n",
    "    \n",
    "    # Update generation_config with any provided kwargs\n",
    "    generation_config.update(kwargs)\n",
    "    \n",
    "    response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough of the Code\n",
    "\n",
    "- We first create a function called `get_completion` that takes in a prompt and any number of keyword arguments called `kwargs`.\n",
    "    - `**kwargs` allows us to pass in any number of keyword arguments to the function.\n",
    "    - the double asterisk is used to unpack the keyword arguments into a dictionary. In JavaScript, this would be `{...args}`.\n",
    "- We then create a `generation_config` dictionary with default values for the parameters we want to default to.\n",
    "- We then update the `generation_config` dictionary with any provided keyword arguments. This allows us to override the default values when we call the function.\n",
    "- We then generate the response from the model using the `generate_content` method on the model dictionary.\n",
    "- Finally, we return the response.\n",
    "\n",
    "Let's call the function with our prompt and print the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(\n",
    "    prompt,\n",
    "    temperature=0.9,\n",
    "    max_output_tokens=1000\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
